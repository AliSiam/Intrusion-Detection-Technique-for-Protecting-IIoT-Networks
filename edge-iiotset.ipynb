{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3319673,"sourceType":"datasetVersion","datasetId":1870444}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing Required Libraries & DataSource\n","metadata":{}},{"cell_type":"code","source":"pip install tensorflow==2.15.0  # for SHAP. Restart the kernel after running this cell","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Libraries for Data Manipulation\nimport pandas as pd\nimport numpy as np\n\n# import gc\n\n# Libraries for Data Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# import altair as alt\n# from scipy.stats import skew\nsns.set(style=\"white\", font_scale=1.5)\nsns.set(rc={\"axes.facecolor\":\"#FFFAF0\", \"figure.facecolor\":\"#FFFAF0\"})\nsns.set_context(\"poster\", font_scale=.7)\n# import matplotlib.ticker as ticker\n\n# Libraries to Handle Warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Libraries for Statistical Analysis\n# from scipy import stats\n# from scipy.stats import chi2, chi2_contingency\n\n# Setting Display Options\npd.set_option(\"display.max.columns\", None)\n\n# Machine Learning Algorithms\n# from sklearn.utils.class_weight import compute_class_weight\n# from sklearn.feature_selection import mutual_info_regression\n# from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\n# import joblib\nimport os\nimport random\nimport shap\n\nimport tensorflow as tf\n# from tensorflow.keras.initializers import HeNormal\n# from tensorflow.keras.regularizers import l1,l2\n# import keras_tuner\nimport keras\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n# from sklearn.feature_selection import RFE, RFECV\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, roc_auc_score, roc_curve\n\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Input, ReLU, LeakyReLU, Concatenate, Dense, Dropout, Layer\nfrom tensorflow.keras.layers import BatchNormalization, Activation, Add, GlobalAveragePooling1D\nfrom tensorflow.keras.models import Sequential, Model, load_model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import F1Score\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.initializers import GlorotUniform\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(tf.__version__)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# for reproducibility\ndef reset_random_seeds(seed=42):\n   tf.random.set_seed(seed)\n   np.random.seed(seed)\n   random.seed(seed)\n   tf.keras.utils.set_random_seed(seed)\n   os.environ['PYTHONHASHSEED']=str(seed)\n   os.environ['TF_DETERMINISTIC_OPS'] = '1'\n   os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n\nreset_random_seeds()\n# As described in the TF docs: \"Calling tf.keras.utils.set_random_seed sets the Python seed, the NumPy seed, and the TensorFlow seed.\" So it is not necessary to set them separately.","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1. Load and Inspect Dataset\n","metadata":{}},{"cell_type":"code","source":"data_set = pd.read_csv(\"/kaggle/input/edgeiiotset-cyber-security-dataset-of-iot-iiot/Edge-IIoTset dataset/Selected dataset for ML and DL/DNN-EdgeIIoT-dataset.csv\")\ndata_set.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Remove specific columns for generalization \n","metadata":{"execution":{"iopub.status.busy":"2024-05-31T13:05:30.500546Z","iopub.execute_input":"2024-05-31T13:05:30.501155Z","iopub.status.idle":"2024-05-31T13:05:30.511286Z","shell.execute_reply.started":"2024-05-31T13:05:30.501113Z","shell.execute_reply":"2024-05-31T13:05:30.509512Z"}}},{"cell_type":"code","source":"col_drop=[\"frame.time\", \"ip.src_host\", \"ip.dst_host\", \"arp.src.proto_ipv4\",\"arp.dst.proto_ipv4\", \n\n         \"http.file_data\",\"http.request.full_uri\",\"icmp.transmit_timestamp\",\n\n         \"http.request.uri.query\", \"tcp.options\",\"tcp.payload\",\"tcp.srcport\",\n\n         \"tcp.dstport\", \"udp.port\", \"mqtt.msg\"]\n\ndata_set=data_set.drop(columns=col_drop)\n\n# Replace '0' with 'Normal' and '1' with 'Attack' in Target column\ndata_set['Attack_label'] = data_set['Attack_label'].replace(0, 'Normal')\ndata_set['Attack_label'] = data_set['Attack_label'].replace(1, 'Attack')\n\n# Choose target column\ntarget_column='Attack_label' # Multiclass: Attack_type | Binary: Attack_label\nunique_values = data_set[target_column].value_counts().index.to_numpy()\nprint(unique_values)\ndata_set","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for col in ['http.request.method', 'http.referer', 'http.request.version','dns.qry.name.len','mqtt.conack.flags','mqtt.protoname','mqtt.topic']:\n    data_set[col] = data_set[col].astype('category').cat.codes\n    \ndata_set","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2 | Data Analysis and Basic preprocessing \n","metadata":{}},{"cell_type":"markdown","source":"## 3. **Checking if There's Any Duplicate Records.**","metadata":{}},{"cell_type":"code","source":"print(\"Duplicates in dataset: \",data_set.duplicated().sum())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_set = data_set.drop_duplicates()\nprint(\"Duplicates in dataset: \",data_set.duplicated().sum())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. **Computing Total No. of Missing Values and the Percentage of Missing Values**","metadata":{}},{"cell_type":"code","source":"missing_data = data_set.isnull().sum().to_frame().rename(columns={0:\"Total No. of Missing Values\"})\nmissing_data[\"% of Missing Values\"] = round((missing_data[\"Total No. of Missing Values\"]/len(data_set))*100,2)\nmissing_data","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3 | Exploratory Data Analysis (EDA)","metadata":{}},{"cell_type":"markdown","source":"## **3. Visualising Class Distribution**","metadata":{"execution":{"iopub.status.busy":"2024-05-31T16:19:59.617034Z","iopub.execute_input":"2024-05-31T16:19:59.617478Z","iopub.status.idle":"2024-05-31T16:19:59.675072Z","shell.execute_reply.started":"2024-05-31T16:19:59.61744Z","shell.execute_reply":"2024-05-31T16:19:59.673865Z"}}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef pie_bar_plot(df, col):\n    plt.figure(figsize=(9, 9))\n    \n    # Extract value counts for the specified column\n    value_counts = df[col].value_counts()#.sort_index()\n\n    ax1 = value_counts\n    plt.title(f\"Distribution by {col}\", fontweight=\"black\", size=14, pad=15)\n    colors = sns.color_palette('Set1', len(ax1))\n    plt.pie(ax1.values, labels=None, autopct=\"\", startangle=90, colors=colors)\n    center_circle = plt.Circle((0, 0), 0.4, fc='white')\n    fig = plt.gcf()\n    fig.gca().add_artist(center_circle)\n\n    # Create a legend with labels and values\n    legend_labels = [f\"{label}: {value} ({round(100*value/sum(value_counts),2)}%)\" for label, value in zip(unique_values, value_counts)]\n    plt.legend(legend_labels, loc=\"lower right\", fontsize=8)\n    plt.savefig('Class distribution.png')\n    plt.show()\n\npie_bar_plot(data_set, target_column)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Print class distribution before resampling\nprint(\"Before resampling:\", data_set['Traffic'].value_counts())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4 | Preprocessing\n","metadata":{}},{"cell_type":"markdown","source":"## **4. Splitting the features in dependent and independent features**\n","metadata":{}},{"cell_type":"code","source":"x = data_set.drop(['Attack_label', 'Attack_type'], axis=1)\ny = data_set[target_column]\n\n# Get feature names\nfeature_names = list(x.columns)\n\ny","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **2. Target feature Encoding**","metadata":{}},{"cell_type":"code","source":"reset_random_seeds()\n\nlabel_encoder = LabelEncoder()\nohe = OneHotEncoder(sparse=False, categories=[unique_values])#, categories=[unique_values]\n\nused_encoder=ohe","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"reset_random_seeds()\n\nused_encoder.fit(data_set[[target_column]])\n\n# Override the classes_ attribute with a custom order\nused_encoder.classes_ = np.array(unique_values)  # Custom order\n\n\n# Transform the data\nencoded = used_encoder.transform(data_set[[target_column]])\n\ny = pd.DataFrame(encoded, index=data_set.index,  dtype='int', columns= unique_values  ) # unique_values ['Label'] \n\ny","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"used_encoder.classes_","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pf=y.value_counts()\npdd=pd.DataFrame(pf)\npdd","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dividing in train-test-split\n","metadata":{}},{"cell_type":"code","source":"reset_random_seeds()\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\nx_test, x_valid, y_test, y_valid = train_test_split(x_test, y_test, test_size=0.5, random_state=42)\n\nprint('Train:',x_train.shape, y_train.shape)\nprint('Validation:',x_valid.shape, y_valid.shape)\nprint('Test:', x_test.shape, y_test.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pf=y_train.value_counts()\npdd=pd.DataFrame(pf)\npdd","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **7.Feature Scaling**\n","metadata":{}},{"cell_type":"code","source":"reset_random_seeds()\n\n# scaler = StandardScaler()\nscaler=MinMaxScaler()\nx_train_scaled = scaler.fit_transform(x_train)\nx_valid_scaled = scaler.transform(x_valid)\nx_test_scaled = scaler.transform(x_test)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feature Selection","metadata":{}},{"cell_type":"code","source":"# Feature Selection without normalization\n# x_train=x_train.to_numpy()\n# x_valid=x_valid.to_numpy()\n# x_test=x_test.to_numpy()\n\n# selected_ids=[8,32,34,35,37,38,39,40]\n# x_train_selected=x_train[:,selected_ids]\n# x_valid_selected=x_valid[:,selected_ids]\n# x_test_selected=x_test[:,selected_ids]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Feature Selection with normalization\n# Multilabel Classification\n# selected_ids=[7,8,16,18,19,22,31,34,39,40]  # 10 features based on histogram (frequency of repetition)\n# selected_ids=[7,8,19,22,31,34,39,40] # 8 features\n    \n# Binary Classification\nselected_ids=[6,8,15,32,34,35,37,38,39,40] # 10 features\n# selected_ids=[8,32,34,35,37,38,39,40] # 8 features\n\nx_train_selected=x_train_scaled[:,selected_ids]\nx_valid_selected=x_valid_scaled[:,selected_ids]\nx_test_selected=x_test_scaled[:,selected_ids]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# x_train=x_train\n# x_valid=x_valid\n# x_test=x_test\n\nx_train=x_train_scaled\nx_valid=x_valid_scaled\nx_test=x_test_scaled\n\n# x_train=x_train_selected\n# x_valid=x_valid_selected\n# x_test=x_test_selected","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Deep Learning Modeling","metadata":{}},{"cell_type":"code","source":"# VGG1D\nreset_random_seeds()\n\n# Initialize and train the model\ninput_shape = (x_train.shape[1],1)\nnum_classes = y_train.shape[1]\n\n\ninputs = Input(shape=input_shape)\n# By default, layers like Dense and Conv use random initializations. You can make the weight initialization constant by specifying kernel_initializer.\n# First Conv Block\nxx = Conv1D(filters=64, kernel_size=3, padding='same', activation='relu', kernel_initializer=GlorotUniform(seed=42))(inputs)\nxx = Conv1D(filters=64, kernel_size=3, padding='same', activation='relu', kernel_initializer=GlorotUniform(seed=42))(xx)\nxx = MaxPooling1D(pool_size=2)(xx)\n\n# Second Conv Block\nxx = Conv1D(filters=128, kernel_size=3, padding='same', activation='relu', kernel_initializer=GlorotUniform(seed=42))(xx)\nxx = Conv1D(filters=128, kernel_size=3, padding='same', activation='relu', kernel_initializer=GlorotUniform(seed=42))(xx)\nxx = MaxPooling1D(pool_size=2)(xx)\n\n# Third Conv Block\nxx = Conv1D(filters=256, kernel_size=3, padding='same', activation='relu', kernel_initializer=GlorotUniform(seed=42))(xx)\nxx = Conv1D(filters=256, kernel_size=3, padding='same', activation='relu', kernel_initializer=GlorotUniform(seed=42))(xx)\nxx = MaxPooling1D(pool_size=2)(xx)\n\n# Fourth Conv Block\n# xx = Conv1D(filters=512, kernel_size=3, padding='same', activation='relu')(xx)\n# xx = Conv1D(filters=512, kernel_size=3, padding='same', activation='relu')(xx)\n# xx = MaxPooling1D(pool_size=2)(xx)\n\n# Flatten and Fully Connected Layers\nxx = Flatten()(xx)\nxx = Dense(512, activation='relu', kernel_initializer=GlorotUniform(seed=42))(xx)\nxx = Dropout(0.5)(xx)\nxx = Dense(512, activation='relu', kernel_initializer=GlorotUniform(seed=42))(xx)\nxx = Dropout(0.5)(xx)\noutputs=Dense(num_classes, activation='softmax', kernel_initializer=GlorotUniform(seed=42))(xx) # softmax  sigmoid\n\n# Compile the model\nmodel = Model(inputs, outputs)\n# Compile the model\noptimizer = Adam(learning_rate=0.001)\nmodel.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy','Precision','Recall',F1Score(average='weighted')], jit_compile=False) #'F1Score' binary_crossentropy   categorical_crossentropy","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=True)","metadata":{"trusted":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# For tensorflow 2.15.0\ny_train = y_train.astype('float32')\ny_valid = y_valid.astype('float32')\ny_test = y_test.astype('float32')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 3: Define callbacks\nreset_random_seeds()\n\n# 1. Save the best model during training\ncheckpoint = ModelCheckpoint(filepath='best_model.keras', \n                             monitor='val_f1_score', #val_accuracy\n                             save_best_only=True, \n                             verbose=1, \n                             mode='max')\n\n# 2. Early stopping to avoid overfitting\nearly_stopping = EarlyStopping(monitor='val_loss', \n                               patience=5, \n                               restore_best_weights=True, \n                               verbose=1)\n\n# 3. Reduce learning rate when validation loss plateaus\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', \n                              factor=0.5, \n                              patience=5, \n                              min_lr=0.00001, \n                              verbose=1)\n\nhistory = model.fit(x_train, y_train, \n                         epochs=20, \n                         batch_size=64, #32\n                         validation_data=(x_valid, y_valid),\n                         callbacks=[checkpoint, early_stopping, reduce_lr], \n                         verbose=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 5: Plot the accuracy and loss growth graph\nplt.figure(figsize=(12, 4))\n\n# Plot accuracy\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Train Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Accuracy Growth over Epochs')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n# Plot loss\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Loss Growth over Epochs')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.savefig('accuracy_loss.png')\n\n# plt.show()\n\n# Step 6: Evaluate on the test set\nbest_model = load_model('best_model.keras') #best_model.keras\nmodel1 = best_model\n#model1 = model\n\nmodel1.evaluate(x_test, y_test, verbose=1)\n\n# Step 7: Save the final model and training history\n# model1.save('final_model.h5')  # Save the final model\n\n\n# Step 8: Confusion Matrix\n# Predict on the test set\ny_pred = model1.predict(x_test) \n\nY_pred_classes = np.argmax(y_pred, axis=1)\nY_true_classes = np.argmax(y_test, axis=1)\n\n\nbaseline_score = accuracy_score(Y_true_classes, Y_pred_classes)\n\n# Create the confusion matrix\ncm = confusion_matrix(Y_true_classes, Y_pred_classes)\ncm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\ncm_array_df = pd.DataFrame(cm_normalized, index=unique_values, columns=unique_values) \n\n# Plot the confusion matrix\nfig, ax = plt.subplots(figsize=(15,11))#figsize=(9,7) figsize=(15,11)\nsns.heatmap(cm_array_df, annot=True, ax=ax ,cbar = True, fmt='0.2f', cmap='Blues')\nax.set_title('Confusion Matrix')\nax.set_ylabel('True Label',fontsize=14)\nax.set_xlabel('Predicted Label',fontsize=14)\n\n# # Save confusion matrix as PNG\nplt.savefig('confusion_matrix.png')\n\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"aa=pd.DataFrame(y_test)\naa.reset_index(drop=True, inplace=True)\n# pf=aa.value_counts()\n# pdd=pd.DataFrame(pf)\n# pdd\naa.loc[aa['Password'] == 1]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## SHAP","metadata":{}},{"cell_type":"code","source":"reset_random_seeds()\n\n# Select a background dataset (a subset of training data)\nbackground = x_train[:100]\nbackground = background[..., np.newaxis]  # Convert (100, 41) to (100, 41, 1)\n\n# Create a SHAP explainer\nexplainer = shap.DeepExplainer(model1, background)\n# explainer = shap.Explainer(model1, background)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Select a single sample or multiple samples to explain\n\nclass_id=1 # from 0 to num_classes-1\n\n# For Multiclass classification \n# num_classes=15\n# match class_id:\n#     case 0:\n#         sample_index = 1 # Class 0\n#     case 1:\n#         sample_index = 46 # Class 1\n#     case 2:\n#         sample_index = 3 # Class 2\n#     case 3:\n#         sample_index = 0 # Class 3\n#     case 4:\n#         sample_index = 77 # Class 4\n#     case 5:\n#         sample_index = 2\n#     case 6:\n#         sample_index = 52\n#     case 7:\n#         sample_index = 26 \n#     case 8:\n#         sample_index = 41 \n#     case 9:\n#         sample_index = 139 \n#     case 10:\n#         sample_index = 188\n#     case 11:\n#         sample_index = 83\n#     case 12:\n#         sample_index = 177 \n#     case 13:\n#         sample_index = 601 \n#     case 14:\n#         sample_index = 140 \n\n\n# For Binary classification \nnum_classes=2\nmatch class_id:\n    case 0:\n        sample_index = 1 # Class Normal\n    case 1:\n        sample_index = 0 # Class Attack\n        \n\n\nsample_to_explain = x_test[sample_index:sample_index + 1]\nsample_to_explain = sample_to_explain[..., np.newaxis]\n# Compute SHAP values\nshap_values = explainer.shap_values(sample_to_explain)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"reset_random_seeds()\n\ndata=shap_values[class_id][0]\n\nN = 10\n\n# Get indices sorted by value (descending)\nsorted_indices = sorted(range(len(data)), key=lambda i: data[i], reverse=True)\n\n# Select the first N indices\nindices = sorted_indices[:N]\n\nprint(\"Indices of N largest values:\", indices)\nprint(\"Name of N largest values:\", [feature_names[i] for i in indices])\nprint(\"Values of N largest values:\", [data[i].item() for i in indices])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"True Label\")\nprint(Y_true_classes[sample_index])\nprint(\"Predicted Label\")\nprint(Y_pred_classes[sample_index])\n\n\nplt.figure(figsize=(14, 10))\n\nfor i in range(num_classes):  # num_classes\n    plt.plot(shap_values[i][0], label=unique_values[i])\n\n\nplt.xticks(np.arange(len(feature_names)), feature_names, rotation=90)\nplt.xlabel(\"Features\")\nplt.ylabel(\"SHAP Values\")\nplt.legend(loc='best') #lower right\n\nplt.savefig('SHAP_plot.png')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"True Label\")\nprint(Y_true_classes[sample_index])\nprint(\"Predicted Label\")\nprint(Y_pred_classes[sample_index])\n\nfor i in range(num_classes):\n    \n    plt.figure(figsize=(12, 4))\n    # plt.plot(sample_to_explain[0], label=\"Input Signal\", alpha=0.6)\n    plt.plot(shap_values[i][0], label=\"SHAP Values\", alpha=0.8)\n    plt.title(\"Class:\"+unique_values[i])\n    plt.xlabel(\"Feature\")\n    plt.ylabel(\"Value\")\n    plt.legend()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cross validation","metadata":{}},{"cell_type":"code","source":"#define a function to fit the model\ndef fit_and_evaluate(tr_x,ts_x, tr_y, ts_y):\n    model = None\n    if os.path.exists(\"/kaggle/working/best_model.keras\"):\n        os.remove(\"/kaggle/working/best_model.keras\")\n    model = create_model()\n    results = model.fit(tr_x, tr_y, \n                     epochs=20, \n                     batch_size=64, #32\n                     validation_split=0.2,\n                     callbacks=[checkpoint, early_stopping, reduce_lr], \n                     verbose=1)  \n    print(\"Val Score: \")\n    model = load_model('best_model.keras')\n    model.evaluate(ts_x, ts_y, verbose=1)\n    return results\n\n\nn_folds=5\nepochs=20\nbatch_size=64\n\n#save the model history in a list after fitting so that we can plot later\nmodel_history = [] \nreset_random_seeds()\nscaler=MinMaxScaler()\nx1 = scaler.fit_transform(x)\ny1=y.astype('float32')\nfor i in range(n_folds):\n    print(\"Training on Fold: \",i+1)\n    random_state=np.random.randint(i,1000)\n    print(\"Random state: \",random_state)\n    tr_x, ts_x, tr_y, ts_y = train_test_split(x1, y1, test_size=0.15, \n                                               random_state = random_state)\n    model_history.append(fit_and_evaluate(tr_x,ts_x, tr_y, ts_y))\n    print(\"=======\"*12, end=\"\\n\\n\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# plt.title('Accuracies vs Epochs')\nplt.plot(model_history[0].history['accuracy'], label='Training Fold 1', color='blue')\nplt.plot(model_history[0].history['val_accuracy'], label='Validation Fold 1', color='blue', linestyle = \"dashdot\")\nplt.plot(model_history[1].history['accuracy'], label='Training Fold 2', color='darkorange')\nplt.plot(model_history[1].history['val_accuracy'], label='Validation Fold 2', color='darkorange', linestyle = \"dashdot\")\nplt.plot(model_history[2].history['accuracy'], label='Training Fold 3', color='green')\nplt.plot(model_history[2].history['val_accuracy'], label='Validation Fold 3', color='green', linestyle = \"dashdot\")\nplt.plot(model_history[3].history['accuracy'], label='Training Fold 4', color='violet')\nplt.plot(model_history[3].history['val_accuracy'], label='Validation Fold 4', color='violet', linestyle = \"dashdot\")\nplt.plot(model_history[4].history['accuracy'], label='Training Fold 5', color='red')\nplt.plot(model_history[4].history['val_accuracy'], label='Validation Fold 5', color='red', linestyle = \"dashdot\")\n\n\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(fontsize=13)\nplt.savefig('cross_validation.png')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}